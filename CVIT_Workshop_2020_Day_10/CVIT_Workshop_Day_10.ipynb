{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Base",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "CVIT_Workshop_Day_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xuXQ8YdhM5z",
        "colab_type": "code",
        "outputId": "e48e7afe-21a5-4bb8-b3a1-f112d603642f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "120vlXJQf6Fl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install torch\n",
        "# !pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEI4qvVGf6F2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN1pMQvODq6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_url = '/content/drive/My Drive/CVIT_WORKSHOP_2020/day10'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csIRlqN7yXvG",
        "colab_type": "text"
      },
      "source": [
        "# **CNN & Pytorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwNRRDgWf6GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pp8wY5W1f6GM",
        "colab_type": "code",
        "outputId": "0730543e-ce9c-4b56-ab2b-3d03ba890e50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "trainset = datasets.CIFAR10('./data', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50, shuffle=True)\n",
        "# trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "# trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8UUUraJf6GV",
        "colab_type": "code",
        "outputId": "5adb3da3-6615-4f6e-894d-fdf9e93bfd35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "testset = datasets.CIFAR10('./data', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=50, shuffle=True)\n",
        "\n",
        "# testset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=False, transform=transform)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvw3UgD25sTa",
        "colab_type": "code",
        "outputId": "d13c1fa4-98a2-4acb-ec5f-f343ff2dd52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(testset[1][0].size())\n",
        "for data, labels in testloader:\n",
        "  print(data.shape)\n",
        "  print(labels.shape)\n",
        "  break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 32, 32])\n",
            "torch.Size([50, 3, 32, 32])\n",
            "torch.Size([50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOZrd_zxtni2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        \n",
        "        # Convolution 1\n",
        "        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 1\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "     \n",
        "        # Convolution 2\n",
        "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        # Max pool 2\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        \n",
        "        # Fully connected 1 (readout)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 10) \n",
        "\n",
        "        self.softmax = torch.nn.Softmax(dim = 1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Convolution 1\n",
        "        out = self.cnn1(x)\n",
        "        out = self.relu1(out)\n",
        "        \n",
        "        # Max pool 1\n",
        "        out = self.maxpool1(out)\n",
        "        \n",
        "        # Convolution 2 \n",
        "        out = self.cnn2(out)\n",
        "        out = self.relu2(out)\n",
        "        \n",
        "        # Max pool 2 \n",
        "        out = self.maxpool2(out)\n",
        "        \n",
        "        # Resize\n",
        "        # Original size: (100, 32, 8, 8)\n",
        "        # out.size(0): 100\n",
        "        # New out size: (100, 32*8*8)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        # Linear function (readout)\n",
        "        out = self.fc1(out)\n",
        "        out = self.softmax(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1HWHFoL22e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an object of class MLP\n",
        "model = CNNModel()\n",
        "\n",
        "# Define the loss\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizers require the parameters to optimize and a learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3L2zBUP3lv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 100\n",
        "for e in range(epochs):\n",
        "    print(\"Epoch \", (e + 1))\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model.forward(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        accuracy = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in testloader:\n",
        "                logit = model(images)\n",
        "                ps = logit#torch.exp(logit)\n",
        "                top_p, top_class = ps.topk(1, dim = 1)\n",
        "                equals = top_class == labels.view(*top_class.shape)\n",
        "                accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "        print(\"Loss on training set: \", running_loss)\n",
        "        print(\"Accuracy on test set: \", end = \" \")\n",
        "        print((accuracy / len(testloader)).data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rath0bEp3-Xt",
        "colab_type": "text"
      },
      "source": [
        "## **Task**\n",
        "\n",
        "1. Choose any dataset available online for multi class classification(minimum 3 classes) and try and train a CNN on that dataset.\n",
        "2. You should try and write your own dataloader function and don't use pytorch's inbuilt one.\n",
        "3. Experiment with the number of layers, kernel size, strides, padding.\n",
        "4. Get the best accuracy you can get.\n",
        "5. Don't use the datasets which are inbuilt in pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXnKmIFSEKiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDo7dYuFUc7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = base_url + '/8fruit_dataset'\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "batch_size = 50\n",
        "train_ratio, test_ratio = 0.8, 0.1\n",
        "lr = 0.05\n",
        "epochs = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVpJiQK2DkmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "  train_x, train_y = [], []\n",
        "  valid_x, valid_y = [], []\n",
        "  test_x, test_y = [], []\n",
        "  labels = []\n",
        "\n",
        "  def shuffle_list(xx, yy):\n",
        "    data = list(zip(xx, yy))\n",
        "    random.shuffle(data)\n",
        "    xx, yy = zip(*data)\n",
        "    return xx, yy\n",
        "\n",
        "  for label, folder in enumerate(os.listdir(url)):\n",
        "      labels.append(folder)\n",
        "      folder_path = os.path.join(url, folder)\n",
        "      X, Y = [], []\n",
        "      for file in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "        img = cv2.imread(file_path, cv2.COLOR_BGR2RGB)\n",
        "        X.append(img)\n",
        "        Y.append(label)\n",
        "      X, Y = shuffle_list(X, Y)\n",
        "      train_split = int(train_ratio * len(X))\n",
        "      test_split = int(test_ratio * len(X)) + train_split\n",
        "      train_x.extend(X[:train_split]) \n",
        "      train_y.extend(Y[:train_split])\n",
        "      test_x.extend(X[train_split:test_split]) \n",
        "      test_y.extend(Y[train_split:test_split]) \n",
        "      valid_x.extend(X[test_split:]) \n",
        "      valid_y.extend(Y[test_split:]) \n",
        "\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  train_x = torch.Tensor(train_x)\n",
        "  train_y = torch.Tensor(train_y)\n",
        "  trainset = TensorDataset(train_x, train_y)\n",
        "  trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  valid_x = torch.Tensor(valid_x)\n",
        "  valid_y = torch.Tensor(valid_y)\n",
        "  validset = TensorDataset(valid_x, valid_y)\n",
        "  validloader = DataLoader(validset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  test_x = torch.Tensor(test_x)\n",
        "  test_y = torch.Tensor(test_y)\n",
        "  testset = TensorDataset(test_x, test_y)\n",
        "  testloader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "  return labels, trainloader, validloader, testloader\n",
        "\n",
        "labels, trainloader, validloader, testloader = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z2YYt85IolF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNNModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.cnn1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    self.cnn3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
        "    self.maxpool3 = nn.MaxPool2d(kernel_size=3)\n",
        "\n",
        "    self.fc1 = nn.Linear(32 * 7 * 7, 8)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.cnn1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool1(x)\n",
        "    x = self.cnn2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool2(x)\n",
        "    x = self.cnn3(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool3(x)\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = self.fc1(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vW0OANvfSjjd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CNNModel()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93LNU5OYq1-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ea1c1da-5c32-4afb-94dc-9e455fb7e7b1"
      },
      "source": [
        "for i,l in trainloader:\n",
        "  i = i.permute(0, 3, 1, 2)\n",
        "  print(i.shape)\n",
        "  print(l.shape)\n",
        "  break"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([50, 3, 100, 100])\n",
            "torch.Size([50])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CW1UDFP3SW1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b7d7eeb3-d60c-472a-c752-c783d398e651"
      },
      "source": [
        "for e in range(epochs):\n",
        "  print('Epoch: ' + str(e+1))\n",
        "  running_loss = 0\n",
        "  for image, label in trainloader:\n",
        "    optimizer.zero_grad()\n",
        "    image = image.permute(0, 3, 1, 2)\n",
        "    label = label.type(torch.LongTensor)\n",
        "    logits = model(image)\n",
        "    loss = criterion(logits, label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "  else:\n",
        "    accuracy = 0\n",
        "    with torch.no_grad():\n",
        "      for image, label in validloader:\n",
        "        image = image.permute(0, 3, 1, 2)\n",
        "        label = label.type(torch.LongTensor)\n",
        "        logit = model(image)\n",
        "        ps = logit\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == label.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "    print(\"Loss on training set: \", running_loss)\n",
        "    print(\"Accuracy on validation set: \", end = \" \")\n",
        "    print((accuracy / len(validloader)).data.numpy())"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "Loss on training set:  69.82560873031616\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 2\n",
            "Loss on training set:  69.73945927619934\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 3\n",
            "Loss on training set:  69.73945927619934\n",
            "Accuracy on validation set:  0.14533333\n",
            "Epoch: 4\n",
            "Loss on training set:  69.78660130500793\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 5\n",
            "Loss on training set:  69.73945903778076\n",
            "Accuracy on validation set:  0.14533333\n",
            "Epoch: 6\n",
            "Loss on training set:  69.72374439239502\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 7\n",
            "Loss on training set:  69.77088785171509\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 8\n",
            "Loss on training set:  69.69231581687927\n",
            "Accuracy on validation set:  0.136\n",
            "Epoch: 9\n",
            "Loss on training set:  69.75517296791077\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 10\n",
            "Loss on training set:  69.73945879936218\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 11\n",
            "Loss on training set:  69.75517344474792\n",
            "Accuracy on validation set:  0.14533333\n",
            "Epoch: 12\n",
            "Loss on training set:  69.75517344474792\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 13\n",
            "Loss on training set:  69.73945903778076\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 14\n",
            "Loss on training set:  69.75517344474792\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 15\n",
            "Loss on training set:  69.69231605529785\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 16\n",
            "Loss on training set:  69.7394597530365\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 17\n",
            "Loss on training set:  69.77088761329651\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 18\n",
            "Loss on training set:  69.7237446308136\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 19\n",
            "Loss on training set:  69.77088713645935\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 20\n",
            "Loss on training set:  69.72374391555786\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 21\n",
            "Loss on training set:  69.72374439239502\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 22\n",
            "Loss on training set:  69.6766015291214\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 23\n",
            "Loss on training set:  69.78660082817078\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 24\n",
            "Loss on training set:  69.7237446308136\n",
            "Accuracy on validation set:  0.136\n",
            "Epoch: 25\n",
            "Loss on training set:  69.70802998542786\n",
            "Accuracy on validation set:  0.136\n",
            "Epoch: 26\n",
            "Loss on training set:  69.7080307006836\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 27\n",
            "Loss on training set:  69.7237446308136\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 28\n",
            "Loss on training set:  69.77088713645935\n",
            "Accuracy on validation set:  0.18266669\n",
            "Epoch: 29\n",
            "Loss on training set:  69.72374486923218\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 30\n",
            "Loss on training set:  69.73945951461792\n",
            "Accuracy on validation set:  0.15466668\n",
            "Epoch: 31\n",
            "Loss on training set:  69.6608875989914\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 32\n",
            "Loss on training set:  69.77088761329651\n",
            "Accuracy on validation set:  0.136\n",
            "Epoch: 33\n",
            "Loss on training set:  69.69231629371643\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 34\n",
            "Loss on training set:  69.73945879936218\n",
            "Accuracy on validation set:  0.18266669\n",
            "Epoch: 35\n",
            "Loss on training set:  69.70803034305573\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 36\n",
            "Loss on training set:  69.70803034305573\n",
            "Accuracy on validation set:  0.13599999\n",
            "Epoch: 37\n",
            "Loss on training set:  69.77088737487793\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 38\n",
            "Loss on training set:  69.72374439239502\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 39\n",
            "Loss on training set:  69.73945927619934\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 40\n",
            "Loss on training set:  69.7237446308136\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 41\n",
            "Loss on training set:  69.72374439239502\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 42\n",
            "Loss on training set:  69.77088713645935\n",
            "Accuracy on validation set:  0.14533333\n",
            "Epoch: 43\n",
            "Loss on training set:  69.75517296791077\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 44\n",
            "Loss on training set:  69.72374439239502\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 45\n",
            "Loss on training set:  69.70802998542786\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 46\n",
            "Loss on training set:  69.75517272949219\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 47\n",
            "Loss on training set:  69.73945927619934\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 48\n",
            "Loss on training set:  69.73945832252502\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 49\n",
            "Loss on training set:  69.77088713645935\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 50\n",
            "Loss on training set:  69.72374498844147\n",
            "Accuracy on validation set:  0.14533333\n",
            "Epoch: 51\n",
            "Loss on training set:  69.69231605529785\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 52\n",
            "Loss on training set:  69.70803046226501\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 53\n",
            "Loss on training set:  69.73945808410645\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 54\n",
            "Loss on training set:  69.77088737487793\n",
            "Accuracy on validation set:  0.16399999\n",
            "Epoch: 55\n",
            "Loss on training set:  69.70802974700928\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 56\n",
            "Loss on training set:  69.72374415397644\n",
            "Accuracy on validation set:  0.16399999\n",
            "Epoch: 57\n",
            "Loss on training set:  69.77088665962219\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 58\n",
            "Loss on training set:  69.7237446308136\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 59\n",
            "Loss on training set:  69.6923155784607\n",
            "Accuracy on validation set:  0.14533333\n",
            "Epoch: 60\n",
            "Loss on training set:  69.6923155784607\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 61\n",
            "Loss on training set:  69.77088785171509\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 62\n",
            "Loss on training set:  69.75517320632935\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 63\n",
            "Loss on training set:  69.73945903778076\n",
            "Accuracy on validation set:  0.18266666\n",
            "Epoch: 64\n",
            "Loss on training set:  69.72374439239502\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 65\n",
            "Loss on training set:  69.70803046226501\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 66\n",
            "Loss on training set:  69.70803046226501\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 67\n",
            "Loss on training set:  69.70802974700928\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 68\n",
            "Loss on training set:  69.70803046226501\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 69\n",
            "Loss on training set:  69.70802962779999\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 70\n",
            "Loss on training set:  69.67660129070282\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 71\n",
            "Loss on training set:  69.73945951461792\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 72\n",
            "Loss on training set:  69.70803046226501\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 73\n",
            "Loss on training set:  69.7237446308136\n",
            "Accuracy on validation set:  0.136\n",
            "Epoch: 74\n",
            "Loss on training set:  69.7394585609436\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 75\n",
            "Loss on training set:  69.73945903778076\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 76\n",
            "Loss on training set:  69.67660200595856\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 77\n",
            "Loss on training set:  69.67660176753998\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 78\n",
            "Loss on training set:  69.73945879936218\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 79\n",
            "Loss on training set:  69.67660105228424\n",
            "Accuracy on validation set:  0.136\n",
            "Epoch: 80\n",
            "Loss on training set:  69.77088737487793\n",
            "Accuracy on validation set:  0.14533333\n",
            "Epoch: 81\n",
            "Loss on training set:  69.77088785171509\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 82\n",
            "Loss on training set:  69.73945832252502\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 83\n",
            "Loss on training set:  69.73945927619934\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 84\n",
            "Loss on training set:  69.77088761329651\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 85\n",
            "Loss on training set:  69.73945951461792\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 86\n",
            "Loss on training set:  69.78660142421722\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 87\n",
            "Loss on training set:  69.7394585609436\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 88\n",
            "Loss on training set:  69.78660082817078\n",
            "Accuracy on validation set:  0.18266666\n",
            "Epoch: 89\n",
            "Loss on training set:  69.6766015291214\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 90\n",
            "Loss on training set:  69.69231629371643\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 91\n",
            "Loss on training set:  69.77088737487793\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 92\n",
            "Loss on training set:  69.72374415397644\n",
            "Accuracy on validation set:  0.13599999\n",
            "Epoch: 93\n",
            "Loss on training set:  69.73945951461792\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 94\n",
            "Loss on training set:  69.73945903778076\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 95\n",
            "Loss on training set:  69.7080307006836\n",
            "Accuracy on validation set:  0.17333333\n",
            "Epoch: 96\n",
            "Loss on training set:  69.75517272949219\n",
            "Accuracy on validation set:  0.14533332\n",
            "Epoch: 97\n",
            "Loss on training set:  69.70802927017212\n",
            "Accuracy on validation set:  0.164\n",
            "Epoch: 98\n",
            "Loss on training set:  69.7394585609436\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 99\n",
            "Loss on training set:  69.72374439239502\n",
            "Accuracy on validation set:  0.15466666\n",
            "Epoch: 100\n",
            "Loss on training set:  69.77088725566864\n",
            "Accuracy on validation set:  0.14533333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPdbm3VAc8kE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7752d1f5-7681-4fd6-fac9-107b5b9f6e46"
      },
      "source": [
        "accuracy = 0\n",
        "with torch.no_grad():\n",
        "  for image, label in testloader:\n",
        "    image = image.permute(0, 3, 1, 2)\n",
        "    label = label.type(torch.LongTensor)\n",
        "    logit = model(image)\n",
        "    ps = logit\n",
        "    top_p, top_class = ps.topk(1, dim=1)\n",
        "    equals = top_class == label.view(*top_class.shape)\n",
        "    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "print(\"Accuracy on test set: \", end = \" \")\n",
        "print((accuracy / len(testloader)).data.numpy())"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set:  0.16061224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE3uBhg05-FV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}